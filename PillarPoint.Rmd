---
title: "Environmental DNA (eDNA) metabarcoding differentiates between micro-habitats within the rocky intertidal"
author: "Meghan M. Shea"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
#Install & Load Requires Packages
#Check if all required packages have been installed and loaded, and if not, install and load them. 

options(repos = list(CRAN="http://cran.rstudio.com/"))

packages <- c("tidyverse", 
              "sf",
              "patchwork", 
              "stringr", 
              "BiocManager", 
              "remotes", 
              "eulerr", 
              "indicspecies", 
              "cluster",
              "vegan",
              "chisq.posthoc.test", 
              "taxize",
              "Polychrome",
              "pals", 
              "spocc",
              "mregions",
              "rgbif",
              "mapview",
              "ggbeeswarm",
              "ggh4x")

#Packages that I don't think are necessary to load, but I did load at some point: "rsvg", "grImport2", "purrr", "sjmisc", "viridis", "ape", "colourvalues", "usedist", "ggnewscale", "sp"



install.packages(setdiff(packages, rownames(installed.packages())))

if(!require("phyloseq", quietly=TRUE))
  BiocManager::install("phyloseq")

if(!require("ggtree", quietly=TRUE))
  BiocManager::install("ggtree")

if(!require("ggtreeExtra", quietly=TRUE))
  BiocManager::install("ggtreeExtra")

if(!require("ampvis2", quietly=TRUE))
  remotes::install_github("kasperskytte/ampvis2")

packages = append(packages, c("phyloseq", "ampvis2", "ggtree", "ggtreeExtra"))

invisible(lapply(packages, library, character.only = TRUE))

install_version("tidytree", version = "0.4.2")
library(tidytree)

```

### Materials and Methods

#### 2.1 Detailed Protocols

To improve reproducibility (e.g. Dickie et al., 2018; Shea et al., 2023) and aid in the initiation of new eDNA biomonitoring projects, we have published detailed, step-by-step protocols for many of the methods described, including specific materials used, photographs, and additional methodological notes not possible to include here. See Shea (2023) for sample collection and filtering, Shea (2023) for DNA extractions, Shea (2023) for PCR amplification, and Shea (2023) for shipping samples. 

#### 2.2 Sampling Site

To better understand the spatial and temporal scales of eDNA detections in a complex coastal environment, we sought a rocky intertidal field location that had consistent, large, accessible tide pools that were fully isolated from one another at some low tides but interconnected during other parts of their exposure period. We selected the intertidal at Pillar Point, a headlands promontory to the west of Pillar Point Harbor in San Mateo County, California, USA. Pillar Point is a popular recreational intertidal site that is directly adjacent to the Pillar Point State Marine Conservation Area (no specific use scientific collection permit required). 

Within Pillar Point, we sampled at three discrete locations: two individual tide pools with a range of physical connectivity (Tide Pool 1, S1: 37.495342°, -122.498731°; Tide Pool 2, S2: 37.495000°, -122.498947°) and an equidistant location (Nearshore, N: 37.4952833°, -122.4992056°) where there was well-mixed offshore water for the duration of the tidal cycle (Figure 2; not produced in R). Each site was approximately 40 meters from all other sites. Tide Pool 1 and Tide Pool 2 are fully isolated at tidal heights of around 0 m (mean low low water, MLLW) or lower, and substantially connected at around 0.25 m or higher. On the day we sampled, this meant water actively flowed between the locations at the start (11:30 am PST) and end (17:00 pm PST) of the sampling period, but that the sites were disconnected at low tide in the middle of the sampling period. 

#### 2.3 Sample Collection & Filtration

We collected 1 L surface samples from each location every 30 minutes for the duration of time the intertidal was exposed on 28 January 2022, using single-use enteral feeding pouches (Covidien, Dublin, Ireland). Sampling commenced at 11:30 am PST; the exact times of sample collection, in relation to the tide, are shown in Figure 3. Following the approach used by Gold et al. (2021b), we attached a sterile 0.22 um pore size Sterivex cartridge (MilliporeSigma, Burlington, MA, USA) to the tubing of each feeding pouch, allowing samples to be immediately gravity filtered in the field. While gravity filtering (1-2 hours per sample), samples were shaded with an awning to prevent any degradation by sunlight (Andruszkiewicz et al., 2017). 


```{r Figure 3}

#Read Pillar Point NOAA/NOS/CO-OPs Tide Chart data file
Tides = read.table("Data/28-Jan-2022-TideChart.txt", header = TRUE, sep = "", dec = ".", skip=13)

#Convert date format in table using lubridate
Tides$datetime <- lubridate::ymd_hm(paste(Tides$Date, Tides$Time), tz = "US/Pacific")

#Set bounds of sampling period to highlight
xmin = Tides$datetime[47]
xmax = Tides$datetime[69]
ymin = -.5
ymax = .5

box_color = "#EDEDED"

#Create plot of full tidal cycle with sampling period highlighted
p1 <- ggplot(data=Tides, aes(x=datetime, y = Pred, group=1)) + 
  annotate("rect", xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = box_color) +
  geom_line() +
  coord_cartesian(xlim = c(Tides$datetime[1], Tides$datetime[96])) +
  theme_classic(base_size = 16) +
  theme(
    panel.background = element_rect(fill = "transparent", colour = NA_character_), 
    # necessary to avoid drawing panel outline
    plot.background = element_rect(fill = "transparent", colour = NA_character_),
    # necessary to avoid drawing plot outline
    axis.text.x = element_text(color="black"),
    axis.ticks = element_line(color = "black"),
    axis.text.y = element_text(color="black"),
    panel.border = element_rect(colour = "black", fill=NA, linewidth=1)
  ) +
  xlab("Time (PST)") +
  ylab("MLLW Tide Prediction (m)") +
  scale_x_datetime(date_breaks = "2 hour", date_labels = "%H:00", expand = c(0, 0), position = "top")

#Create plot of just sampling period
p2 <- ggplot(data=Tides, aes(x=datetime, y = Pred, group=1)) + 
  geom_line() +
  coord_cartesian(xlim = c(xmin, xmax), ylim = c(ymin, ymax)) +
  theme_classic(base_size = 16) + 
  theme(panel.background = element_rect(fill = box_color), 
        plot.background = element_rect(fill = "transparent", colour = NA_character_), 
        axis.text.x = element_text(color="black"),
        axis.ticks = element_line(color = "black"),
        axis.text.y = element_text(color="black"),
        panel.border = element_rect(colour = "black", fill=NA, linewidth=1)) +
  xlab("") +
  ylab("MLLW Tide Prediction (m)") +
  scale_x_datetime(date_breaks = "30 min", date_labels = "%H:%M", expand = c(0, 0))

#Stitch plots together using patchwork
p_patch <- p1 / p2 & ylab(NULL) & theme(plot.margin = margin(5.5, 5.5, 5.5, 0), plot.background = element_rect(fill = "transparent", colour = NA_character_))

# Use the tag label as a y-axis label
p_patch = wrap_elements(p_patch) +
  labs(tag = "MLLW Tide Prediction (m)") +
  theme(
    plot.tag = element_text(size = rel(1.2), angle = 90),
    plot.tag.position = "left",
    plot.background = element_rect(fill = "transparent", colour = NA_character_)
  )

p_patch

ggsave(
  plot = p_patch,
  filename = "Figures/Figure3.png",
  bg = "transparent"
)


##Second section of plot continued below once data has been loaded
```

At three time points (see Figure 3; additional items added outside of R) at the beginning and end of the sampling period as well as at low tide, we collected triplicate 1 L samples from each location as biological replicates. At the beginning and end of the sampling period, we also filtered 1 L MilliQ water via the procedure described above to serve as negative field controls. Additionally, using an Orion Model 1230 meter (Orion Research Inc., Beverly, MA, USA), we recorded temperature and salinity in each location directly after samples were collected. 

Once finished filtering, Sterivex cartridges were dried by pushing air through them using a sterile 3 mL syringe, capped, placed in sterile Whirl-Pak bags (Whirl-Pak, Madison, WI, USA). Then, samples were stored in a cooler on ice until transported back to the laboratory at the end of the sampling period. Samples were transferred to a -20C freezer for up to 18 days, at which time they were processed to extract nucleic acids from the captured materials.  

#### 2.4 DNA Extraction & Library Preparation

Within 18 days of collection, we extracted DNA from the Sterivex cartridge using the DNeasy Blood and Tissue Kit (Qiagen, Germantown, MD, USA) and the modifications described in Spens et al. (2017). In short, we incubated the filter cartridge overnight with proteinase K and ATL. Then, we extracted the liquid from the cartridge with a syringe and mixed it with equal volumes of AL buffer and 0C ethanol before proceeding with the manufacturer’s extraction protocol. One negative extraction control (DNA-grade water in place of a sample) was included in each of four batches of extractions. Extracted samples to be PCR amplified were stored at -20C for up to 6 months. 

We PCR amplified the extracted DNA in triplicate within 6 months of storage, using the mlCOIintF/ jgHCO2198 primer set targeting a 313 bp fragment of the mitochondrial COI region optimized by Leray et al. (2013) (Forward: GGWACWGGWTGAACWGTWTAYCCYCC; Reverse: TAIACYTCIGGRTGICCRAARAAYCA) with Nextera modifications. Following Curd et al. (2019), we used a 25 l PCR reaction mixture consisting of 12.5 l of Qiagen Multiplex Mix (Qiagen, Germantown, MD, USA), 2.5 l each of the forward and reverse primers at 2 M (Integrated DNA Technologies, Inc., Coralville, IA, USA), 6.5 l of PCR-grade water, and 1 l of undiluted DNA template. The PCR thermocycling touchdown profile began with an initial denaturation at 95 for 15 minutes to activate the DNA polymerase, followed by 13 cycles of denaturation (94 for 30 seconds), annealing (starting at 69.5 for 30 seconds, with the temperature decreased by 1.5 each cycle), and extension (72 for 1 minute). Then, an additional 35 cycles were run with the same denaturation and extension steps as above with an annealing temperature of 50, followed by a final extension at 72 for 10 minutes. PCR reactions were prepared in a designated DNA-free hood until the template was added. 

PCR amplification was conducted in two batches; in each batch, we included one no-template negative PCR control (DNA-grade water used as template). Additionally, we extracted DNA from tissue from five organisms across a range of phyla we expected to amplify with the mlCOIintF/ jgHCO2198 primers, but not expected to be present at Pillar Point in particular (Mytilus edulis, Mizuhopecten yessoensis, Xiphias gladius, Mercenaria mercenaria, Lutjanus campechanus) using the standard tissue extraction protocol detailed in the DNeasy Blood and Tissue Kit (Qiagen, Germantown, MD, USA). These tissues were obtained from a local grocery store and it was assumed that they were labeled correctly, although previous work has indicated mislabeling in seafood stores can occur (e.g. Willette et al., 2017). Extracts from the 5 tissue samples were combined in equimolar amounts to form a mock community used as a positive PCR control in each batch. Triplicate PCR amplicons from both samples and controls were not subsequently pooled, but were carried through the remaining library preparation and sequencing steps as technical replicates. We electrophoresed and visualized a subset of PCR products on a 1.5% agarose gel stained with GelRed® (Biotium, Fremont, CA, USA) to ensure successful amplification and correct product sizes as well as lack of contamination. 

Post-PCR library preparation and sequencing was conducted at the Georgia Genomics and Bioinformatics Core (GGBC, UG Athens, GA, RRID:SCR_010994). In short, provided PCR amplicons were cleaned using AMPure XP magnetic beads (Beckman Coulter, Indianapolis, IN, USA), barcoded with Nextera adapters (Illumina, San Diego, CA, USA) during a second PCR (3 min at 95 °C; 15 cycles of 30 sec at 95 °C, 30 sec at 67 °C, and 30 sec at 72 °C; and 4 min at 72 °C), cleaned again using AMPureXP magnetic beads, and pooled in equimolar ratio. The resulting library was sequenced on a MiSeq PE 2x250bp (500 cycles) using Reagent Kit V2 with 25% PhiX spike-in (Illumina, San Diego, CA, USA). Given our technical replication of samples and controls, our final library included 6 negative field controls (1 at beginning and end of field sampling, amplified in triplicate), 12 negative extraction controls (1 in each of 4 extraction sets, amplified in triplicate), 2 positive PCR controls (1 in each of 2 amplification batches), 2 no-template negative PCR controls (1 in each of 2 amplification batches), and 159 samples (53 field samples, amplified in triplicate). 

#### 2.5 Bioinformatics

We processed sequencing data using the Anacapa Toolkit, which contains two core modules: one for quality control and ASV parsing, and one for classifying taxonomy (Curd et al., 2019). Briefly, we ran the first module using default parameters, which uses cutadapt (version 1.16) (Martin, 2011) for adapter and primer trimming, FastX-Toolkit (version: 0.0.13) (citation) for quality trimming, and dada2 (version 1.6) (Callahan et al., 2016) for assigning ASVs. For the second module, we utilized the MIDORI2 reference database, a quality controlled and updated database built from GenBank release 253 (12/20/2022) that has been technically validated (Leray et al., 2022). Following Gold et al. (2022), we adjusted the identity and query coverage to 95% (default: 80%) to account for the relative incompleteness of the broad COI reference database compared to more taxonomically-specific databases (Curd et al., 2019). The second module relies on Bowtie 2 (version 2.3.5) (Langmead and Salzberg, 2012) and a modified instance of BLCA (Gao et al., 2017) as dependencies. Following Gold et al. (2021a) we only kept taxonomic assignments that had a bootstrap confidence cutoff score of 60 or higher in BLCA, to avoid spurious assignments from the incomplete reference database. We modified the Anacapa Container (Ogden, 2018), a Singularity container with all the needed dependencies for executing the Anacapa Toolkit, to enable the pipeline to be run in a high-performance computing environment requiring two-step authentication; the updated container, scripts, and reference database with the required Bowtie 2 index library needed to reproduce our bioinformatics process can be found on Dryad (link/citation).  

Raw ASVs, taxonomy assignments, and metadata were converted into interchangeable ampvis2 (version 2.7.34) (Andersen et al., 2018) and phyloseq (version 1.42.0) (McMurdie and Holmes, 2013) objects in R (version 4.2.2) to facilitate decontamination and further analyses. Singletons were removed using ampvis2, and samples were further decontaminated using phyloseq by removing all ASVs that appeared in any negative field control, negative extraction control, or no-template negative PCR control, a choice made due to the very low number of overlapping ASVs between samples and negative controls (Supplemental Information X). Samples were rarified to the minimum number of reads of any sample using ampvis2. Subsequent analyses were robust to all three decontamination steps. 

```{r Core Functions}

#Function that removes NCBI Taxonomy IDs from the end of a taxa name, from an Anacapa Output, in list form 
remove_NCBInums = function(list) {
  list = sub("_[^_]+$","", list)
  return(list)
}

#Function that converts an ampvis2 object to a phyloseq object
#Can be used either on an existing ampvis object (abund, tax, md = NULL) or separate abund, tax, and metadata data frames (ampvis = NULL)
amp_to_phyloseq = function(ampvis, abund, tax, md) {
  if(!is.null(ampvis)) {
    physeq = phyloseq(otu_table(as.matrix(ampvis$abund), taxa_are_rows = TRUE), 
                    tax_table(as.matrix(ampvis$tax)), 
                    sample_data(ampvis$metadata))
  } else if (is.null(md)) {
    physeq = phyloseq(otu_table(as.matrix(abund), taxa_are_rows = TRUE), 
                    tax_table(as.matrix(tax)))
} else {
  physeq = phyloseq(otu_table(as.matrix(abund), taxa_are_rows = TRUE), 
                    tax_table(as.matrix(tax)), 
                    sample_data(md))
}
  return(physeq)
}

```



```{r Importing Anacapa, warning = FALSE}

#Read in Anacapa Output; change p_confidence to switch to a different percent confidence output

current_subfolder = "Data/Anacapa Pipeline Output/Tide Pool 5184 - 31 January 2023"
p_confidence = 60
filepath = paste0(current_subfolder, 
                  "/CO1/CO1_taxonomy_tables/Summary_by_percent_confidence/", 
                  p_confidence, 
                  "/CO1_ASV_raw_taxonomy_", 
                  p_confidence, 
                  ".txt")

AnacapaOutput = read.table(filepath, header = TRUE, sep = "\t", dec = ".")


#Format ASV Table (extract just the needed information)

endcol = ncol(AnacapaOutput) - 1
asvmat = AnacapaOutput[,2:endcol]
rownames(asvmat) <- AnacapaOutput[,1]

#Format Taxonomy Table (extract just the needed information and rename columns)

endcol = ncol(AnacapaOutput)
taxmat = AnacapaOutput[,endcol]
taxmat = str_split_fixed(taxmat, ";",7)

colnames(taxmat) <- c("Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")

rownames(taxmat) <- rownames(asvmat)

#Read Metadata (saved in multiple different files)

current_subfolder = paste0(current_subfolder, "/Metadata-Added/PillarPoint_SampleInfo_") 

filepath = paste0(current_subfolder, "EnviroData.txt")
EnviroData = read.table(filepath, header = TRUE, sep = "\t", dec = ".", strip.white = TRUE)

filepath = paste0(current_subfolder, "FieldSampling.txt")
FieldSampling = read.table(filepath, header = TRUE, sep = "\t", dec = ".", strip.white = TRUE)
FieldSampling = FieldSampling[,1:4]

filepath = paste0(current_subfolder, "Sequencing.txt")
Sequencing = read.table(filepath, header = TRUE, sep = "\t", dec = ".", strip.white = TRUE)

filepath = paste0(current_subfolder, "Location.txt")
Location = read.table(filepath, header = TRUE, sep = "\t", dec = ".", strip.white = TRUE)

#Format Metadata

sampledata = merge(FieldSampling, EnviroData, all=T)
sampledata = sampledata %>% drop_na(Dereplicated_Sample_Name) 
#removing one sample that has EnviroData reading but the field sample was not processed

sampledata = merge(Sequencing,sampledata, all=T)

rownames(sampledata) <- sampledata[,"X"]

sampledata$Dereplicated_Sample_Name = as.factor(sampledata$Dereplicated_Sample_Name)
sampledata$Type = as.factor(sampledata$Type)
sampledata$PCR = as.factor(sampledata$PCR)
sampledata$Time = as.factor(sampledata$Time)
sampledata$Location = as.factor(sampledata$Location)
sampledata$Extraction = as.factor(sampledata$Extraction)
sampledata$SiteByTime = as.factor(paste(sampledata$Location, sampledata$Time))

#Make ampvis2 objects without any pre-processing
asvmat_amp = tibble::rownames_to_column(asvmat, "ASV")
sampledata_amp = tibble::rownames_to_column(sampledata, "Sample")
taxmat_amp = tibble::rownames_to_column(as.data.frame(taxmat), "ASV")
amp = amp_load(asvmat_amp, metadata = sampledata_amp, taxonomy = taxmat_amp)

amp_nocontrols = amp_filter_samples(amp, Type %in% c("sample"))
amp_controls = amp_filter_samples(amp, Type %in% c("control"))

#Make ampvis2 objects with singletons removed
amp_NS = amp_load(asvmat_amp, metadata = sampledata_amp, taxonomy = taxmat_amp, pruneSingletons = TRUE)

amp_nocontrols_NS = amp_filter_samples(amp_NS, Type %in% c("sample"))
amp_controls_NS = amp_filter_samples(amp_NS, Type %in% c("control"))


#Make ampvis2 objects with a full decontamination procedure 

#Function that removes any ASV that appears in any control from all samples
full_decontamination <-function(amp) {
  
  physeq = amp_to_phyloseq(amp, NULL, NULL, NULL)
  
  physeq_controls <- physeq %>%
    subset_samples(Type == "control") %>%
    prune_taxa(taxa_sums(.) >0, .)
  
  badASV = taxa_names(physeq_controls)
  allASV = taxa_names(physeq)
  allASV <- allASV[!(allASV %in% badASV)]
  physeq_FD = prune_taxa(allASV, physeq)
  
  av2_otutable <- data.frame(otu_table(physeq_FD)@.Data)
  av2_taxtable <- data.frame(tax_table(physeq_FD)@.Data)
  av2_metadata <- data.frame(sample_data(physeq_FD))
  
  amp_return <- amp_load(av2_otutable, metadata = av2_metadata, taxonomy = av2_taxtable)
  amp_return= amp_filter_samples(amp_return, Type %in% c("sample"))
  
  return(amp_return)
}

amp_nocontrols_FD = full_decontamination(amp)

amp_nocontrols_NS_FD = full_decontamination(amp_NS)


#Make ampvis2 objects by rarefying to the minimum number of reads of any sample

#Function that wraps around an ampvis2 function to rarefy samples to the minimum number of reads of any sample
rarefaction <- function(amp) {
  set.seed(0)
  reads <- colSums(amp$abund)
  minreads <- min(reads)
  amp_R = amp_subset_samples(amp, rarefy = minreads)
  return(amp_R)
}
  
amp_nocontrols_R = rarefaction(amp_nocontrols)

amp_nocontrols_NS_R = rarefaction(amp_nocontrols_NS)

amp_nocontrols_FD_R = rarefaction(amp_nocontrols_FD)

amp_nocontrols_NS_FD_R = rarefaction(amp_nocontrols_NS_FD)


##Create data frame for GBIF
#Exporting version of data with decontamination and controls removed but no additional processing

GBIF_export = amp_nocontrols_FD

GBIF_export$metadata = merge(GBIF_export$metadata, Location, all=T)

TimeZone = "UTC−08:00"
Date = "28 January 2022"
GBIF_export$metadata = cbind(GBIF_export$metadata, TimeZone)
GBIF_export$metadata = cbind(GBIF_export$metadata, Date)

names(GBIF_export$metadata)[names(GBIF_export$metadata) == 'Sample'] <- 'Sequencing_Sample_Name'

col_order = c("Sequencing_Sample_Name", "Sample_Name", "Location", "Latitude", "Longitude", "CRS", "Date", "Time", "TimeZone", "T", "S")
GBIF_export$metadata = GBIF_export$metadata[,col_order]

#Add sequences to file

current_subfolder = "Data/Anacapa Pipeline Output/Tide Pool 5184 - 31 January 2023"
filepath = paste0(current_subfolder, 
                  "/CO1/CO1_taxonomy_tables/CO1_ASV_taxonomy_detailed.txt")

AnacapaDetailed = read.table(filepath, header = TRUE, sep = "\t", dec = ".")
sequences = as.data.frame(AnacapaDetailed[,"sequence"])
rownames(sequences) = AnacapaDetailed$CO1_seq_number

GBIF_export$tax = merge(sequences, GBIF_export$tax, by = "row.names", all.x = FALSE, all.y = TRUE, sort = FALSE)
rownames(GBIF_export$tax) = GBIF_export$tax$Row.names
names(GBIF_export$tax)[names(GBIF_export$tax) == "AnacapaDetailed[, \"sequence\"]"] <- 'Sequence'

col_order = c("Sequence", "Kingdom", "Phylum", "Class", "Order", "Family", "Genus", "Species")
GBIF_export$tax = GBIF_export$tax[,col_order]

write.csv(GBIF_export$abund, "Data/GBIF/ASVTable.csv", row.names=TRUE)
write.csv(GBIF_export$tax, "Data/GBIF/TaxTable.csv", row.names=TRUE)
write.csv(GBIF_export$metadata, "Data/GBIF/SampleData.csv", row.names=FALSE)


```

To ensure the accuracy of taxonomic assignments, we analyzed occurrence data from the Global Biodiversity Information Facility to investigate whether identified species had occurrence records in the California Current System and known ranges that encompassed Pillar Point, using the spocc (version 1.2.0) (Chamberlain, 2021) package. A phylogenetic tree based on taxonomic assignments was created using the taxize (version 0.9.100) (Chamberlain and Szöcs, 2013) and treedataverse (version 0.0.1) (citation) packages. 

#### Data Analysis

To understand whether eDNA signals could be distinguished by location, we first analyzed individual-level differences between locations; that is, whether ASVs or individual taxa (agglomerated species-level taxonomic assignments) were unique to, or associated with, particular locations. We calculated and visualized unique ASVs and taxa using the nVennR (version 0.2.3) (Pérez-Silva et al., 2018) and eulerr (version 7.0.0) (Larsson, 2022) packages. However, with metabarcoding data in particular, taxa or ASVs that are unique to a given location are not necessarily ecologically meaningful; they could include rare taxa present elsewhere but not amplified and exclude taxa that are well correlated with particular locations but sometimes detected at others. Thus, we also analyzed ASVs and taxa using an indicator species framework (Dufrêne and Legendre, 1997). In this framework, the null hypothesis is that the frequency of a taxon’s or ASV’s presence in samples from a particular location is not higher that the frequency of that taxon’s or ASV’s presence in samples from other locations. For each location, we identified all statistically-significant indicator taxa and ASVs with indicator value indices above 0.7—that is, indicators that are well-associated with a site group even if they are detected in samples from other sites—based on presence-absence data per sample using the indicspecies package (version 1.7.12) (Cáceres and Legendre, 2009). 

We followed our individual-level analyses with approaches to test whether community composition varied by location. We calculated a Jaccard dissimilarity matrix across all samples using vegan (version 2.6.4) (Oksanen et al., 2022). Then, we tested for differences in community composition among locations using a permutational multivariate analysis of variance (PERMANOVA) with the model eDNA Presence ~ Location + Time + Biological Replicates using the adonis function in vegan. We confirmed that locational differences found via PERMANOVA were not a result of differences in dispersions by testing for homogeneity of dispersions using the betadisper function in vegan. We also visualized Jaccard dissimilarity using non-metric multidimensional scaling (NMDS) using the metaMDS function in vegan. We coupled these analyses with a partitioning among medoids algorithm (Kaufman and Rousseeuw, 1990) implemented with the pam function in the cluster package (version 2.1.4) (Maechler et al., 2022). Rather than assuming location clusters a priori, we validated the optimal number of clusters for a given dataset by finding the number of clusters (k) that maximized the average silhouette width, a measure of how well-structured the clusters are. Finally, to better understand how the difference between locations varied over the time sampled, for each time point, we calculated the Jaccard dissimilarity between each unique combination of replicates within each site, and then the pairwise Jaccard dissimilarity between each unique combination of samples across the three pairs of sites: S1-N, S2-N, and S1-S2.

To further understand whether differences in eDNA detections corresponded with underlying ecological gradients, we compared the unique and indicator taxa identified at each site to their ecological zonation in a highly regarded field guide to the Pacific intertidal, Between Pacific Tides (Ricketts et al., 1985). To account for potential variations in taxonomic names between Between Pacific Tides and the MIDORI2 reference database, we used the World Register of Marine Species (WoRMS) to identify all synonymized names for each unique and indicator taxon identified, and we searched Between Pacific Tides for all synonyms. We compared the proportion of high and middle intertidal species identified across each location using a chi-squared test with p-values computed via Monte Carlo simulation. 

### Results

#### 3.1 Sequencing Results & Taxonomic Diversity


```{r}

#Gather information about full Anacapa Toolkit results before any processing
total_ASVs = nrow(amp$abund)
total_reads = sum(amp$abund)

samples = length(amp$metadata$Type[amp$metadata$Type == "sample"])
controls = length(amp$metadata$Type[amp$metadata$Type == "control"])

#Check positive PCR controls
amp_MC = amp_filter_samples(amp, Location %in% c("MC"))

df = merge(amp_MC$abund, amp_MC$tax, by = 'row.names')
df = df[,c(2:3, 10)]
colnames(df) = c("MC1", "MC2", "Species")
agg_df <- df %>% group_by(Species) %>% summarise(MC1_Reads = sum(MC1), MC2_Reads = sum(MC2)) %>% arrange(desc(MC1_Reads))
agg_df[agg_df == ""] = "No Taxonomic Assignment"

agg_df

amp_no_MC = amp_filter_samples(amp, !(Location %in% c("MC")))
intersect(rownames(amp_MC$abund), rownames(amp_no_MC$abund))

#Check intersection between samples and controls
intersect = intersect(rownames(amp_controls$abund), rownames(amp_nocontrols$abund))
overlap = length(intersect)

overlap_table = list(ASV = intersect, "Taxonomic Assignment" = amp_nocontrols$tax[amp_nocontrols$tax$OTU %in% intersect,]$Species)

overlap_table$`Taxonomic Assignment` = remove_NCBInums(overlap_table$`Taxonomic Assignment`)

overlap_table

write.csv(overlap_table, "Data/Analysis Products/OverlapASVs.csv", row.names=TRUE)

```
Using the Anacapa Toolkit, we identified `r total_ASVs` ASVs from `r total_reads` reads across `r samples` samples and `r controls` controls (positive PCR controls, negative field controls, negative extraction controls, and no-template negative PCR controls). All expected taxa amplified in each positive PCR control, and no ASVs present in the positive PCR controls occurred in any other samples, providing no evidence of index hopping. Before application of any decontamination steps, only `r overlap` ASVs were shared between the samples and negative field, extraction, and PCR controls. 

```{r}
#Establish processed data set used throughout analyses

data_amp = amp_nocontrols_NS_FD_R

data_physeq = amp_to_phyloseq(data_amp, NULL, NULL, NULL)

data_amp_taxa = data_amp
data_amp_taxa$abund = aggregate_abund(data_amp$abund, data_amp$tax, tax_aggregate = "Species", format = "abund")

```


```{r Figure 3 --Continued}

##IN PROGRESS

data_forchart = amp$metadata[!is.na(amp$metadata$Time),]

data_forchart = data_forchart %>% group_by(SiteByTime) %>% mutate(chart_num = ceiling(row_number()/3))

data_forchart$datetime <- lubridate::ymd_hm(paste("2022/01/28", data_forchart$Time), tz = "US/Pacific")
data_forchart$LocNumTime = as.factor(paste(data_forchart$Location, data_forchart$chart_num, data_forchart$Time))


facet.labs =  c("Tide Pool 1\n (S1)", "Tide Pool 2\n (S2)", "Nearshore\n (N)", "Field Blank")
names(facet.labs) = c("S1", "S2", "N", "FB")

data_forchart$Location= factor(data_forchart$Location, levels=c("S1", "S2", "N", "FB"))
data_forchart$chart_num= factor(data_forchart$chart_num, levels=c("3", "2", "1"))

sample_schema = ggplot(data = data_forchart, aes(x = Time, y = chart_num, shape = Location,
                                 color = Time, 
                                 fill = after_scale(alpha(colour, 0.05)))) + 
  geom_beeswarm(cex = 2, size = 4) + 
  geom_box(aes(xmin = after_stat(x) - 0.45, 
               xmax = after_stat(x) + 0.45,
               ymin = after_stat(y) - 0.4, 
               ymax = after_stat(y) + 0.45),
           radius = unit(5, "pt")) +
  scale_shape_manual(values = c(15, 17, 16, 18)) + 
  scale_color_viridis_d(direction = -1) +
  facet_grid(Location ~., scales = "free_y", space = "free_y", 
             switch = "y", labeller = labeller(Location = facet.labs)) +
  theme_void(base_size = 16) + 
  theme(legend.position = "none",
        strip.text.y.left = element_text(angle = 0, hjust = 1, vjust = .9))  


graph = sample_schema 

filename = paste("sample_schema_full.pdf")
  path = paste("Figures/PowerPointFigure/", filename)
  pdf(path, width = 13, height = 4.3)
  print(graph)
  dev.off()
  
  graph$layers[[1]] <- NULL
  
  alpha=c(0,0,0,0,0,0,0,0,0,0,0,0)
 
for (i in 1:12) {
  alpha[i] = 1
  graph <- graph + 
    scale_color_viridis_d(direction = -1, alpha = alpha) +
     scale_fill_viridis_d(direction = -1, alpha = alpha)
  filename = paste("sample_schema", i, ".pdf")
  path = paste("Figures/PowerPointFigure/", filename)
  pdf(path, width = 13, height = 4.3)
  print(graph)
  dev.off()
}


##Still need to line up axes between two plots, probably by changing date format for both
##Still need to add field blank to plots
p_patch/sample_schema







#OLD VERSION

#sample_schema = ggplot(data = data_forchart, aes(x = datetime, y = chart_num, shape = Location, color = Time, fill = Time)) + 
   #facet_grid(Location ~., scales = "free_y", space = "free_y", switch = "y", labeller = labeller(Location = facet.labs)) +
  #geom_point(position = position_dodge2(width = 1)) + 
  #geom_beeswarm(cex= 2, size = 3) +
  #geom_mark_rect(aes(group = LocNumTime)) +
  #coord_cartesian(xlim = c(xmin, xmax)) +
  #scale_shape_manual(values = c(15, 17, 16)) + 
  #viridis::scale_color_viridis(discrete=TRUE, begin = 0, end = .97, direction = -1) +
  #viridis::scale_fill_viridis(discrete=TRUE, begin = 0, end = .97, direction = -1) +
  #theme_void(base_size = 16) + 
  #theme(legend.position = "none") +
#theme(strip.text.y.left = element_text(angle = 0, hjust = 1, vjust = .9)) 




```



```{r}
#Gather information about processed Anacapa Toolkit 

processed_ASVs = nrow(data_amp$abund)
processed_reads = sum(data_amp$abund)

#Create a phyloseq object where all samples are combined into one
total = as.data.frame(rowSums(data_amp$abund))
physeq_summed = amp_to_phyloseq(NULL, total, data_amp$tax[,1:7], NULL)
  
#Summarize the number of unassigned ASVs
kingdom = as.data.frame(tax_table(physeq_summed)[,1])
unassigned_ASVs = kingdom %>% group_by(Kingdom) %>% summarise(n = n()) %>% mutate (freq = n / sum(n))

#Summarize the number of unique phyla
unique_phyla = unique(tax_table(physeq_summed)[,2])
unique_phyla = unique_phyla[unique_phyla != ""]
phyla = length(unique_phyla)

#Summarize the number of unique classes
unique_class = unique(tax_table(physeq_summed)[,3])
unique_class = unique_class[unique_class != ""]
class = length(unique_class)

#Summarize the number of unique orders
unique_order = unique(tax_table(physeq_summed)[,4])
unique_order = unique_order[unique_order != ""]
order = length(unique_order)

#Summarize the number of unique families
unique_family = unique(tax_table(physeq_summed)[,5])
unique_family = unique_family[unique_family != ""]
family = length(unique_family)

#Summarize the number of unique genera
unique_genus = unique(tax_table(physeq_summed)[,6])
unique_genus = unique_genus[unique_genus != ""]
genus = length(unique_genus)
  
#Summarize the number of unique species
unique_species = unique(tax_table(physeq_summed)[,7])
unique_species = unique_species[unique_species != ""]
species = length(unique_species)
  
#Remove all unclassified ASVs and condense to phylum level
physeq_totalreads = tax_glom(physeq_summed, taxrank="Phylum", NArm=TRUE)
Phyla_Reads = data.frame(Phyla = tax_table(physeq_totalreads)[,2], 
                         Reads = rowSums(otu_table(physeq_totalreads)))
    

```


```{r GBIF, cache=TRUE}

#TO-DO: figure out how to set this up with future or callr

#Create list of identified species
species = unique(data_amp$tax$Species)
species = remove_NCBInums(species)
species = species[species != ""]
species = sub("_", "-", species) #to address Pseudo_nitzchia error


#Standardize species names against GBIF backbone
species_GBIF = name_backbone_checklist(species)
species_standardized = species_GBIF$species[!is.na(species_GBIF$species)]
species_both = species_GBIF[c("species", "verbatim_name")]


##Core GBIF checking function
distribution_check <- function(small_bound, big_bound, species, min_records, efficiency) {
  
  ##Function that checks GBIF given a particular geographic region
  boundary_function <- function(species, bounds) {
    no_data = c()
    confirmed = c()
    for (i in species) {
      print(i)
      occ = occ(query = i, geometry = bounds, from = 'gbif', limit = min_records, has_coords = TRUE, gbifopts = list(hasGeospatialIssue = FALSE))
      occ.df = occ2df(occ)
      if (nrow(occ.df) >= min_records) {
        confirmed = append(confirmed, i)
      } else {
        no_data = append(no_data, i)
      }
    }
    return(list(no_data = no_data, confirmed = confirmed))
  }
  
  ##Function that checks GBIF to confirm whether a particular location is within the range of observations
  range_function <- function(species){
    CC.confirmed = c()
    CC.not_confirmed = c()
    CC.no_data = c() 
    limits = c(10, 100, 1000, 10000)
    
    for (i in species) {
      track = 0
      for (l in limits) {
        print(i)
        print (l)
        occ = occ(i, from = 'gbif', limit = l, has_coords = TRUE, gbifopts = list(hasGeospatialIssue = FALSE))
        occ.df = occ2df(occ)
        if (nrow(occ.df) < min_records) {
          CC.no_data = append(CC.no_data, i)
          print("no data")
          track = 1
          break
        }
        bb_occ = sp::bbox(cbind(occ.df$longitude, occ.df$latitude))
        if (bb_occ['x', 'min'] < small_bound[1] & bb_occ['x', 'max'] >small_bound[1] & bb_occ['y', 'min'] <small_bound[4] & bb_occ['y', 'max'] >small_bound[4]) {
          CC.confirmed = append(CC.confirmed, i)
          print("confirmed")
          track = 1
          break
        }
        if(nrow(occ.df) < l) {
          CC.not_confirmed = append(CC.not_confirmed, i)
          print("not confirmed")
          track = 1
          break
        } 
      }
      if(track == 0) {
        CC.not_confirmed = append(CC.not_confirmed, i)
        print("not confirmed")
      }
    }
    return(list(no_data = CC.no_data, not_confirmed = CC.not_confirmed, confirmed = CC.confirmed))
  }
  
  
  #Actual Steps
  PP = boundary_function(species, small_bound)
  
  if(efficiency == TRUE) {
    CC = boundary_function(PP[["no_data"]], big_bound)
  } else {
    CC = boundary_function(species, big_bound)
  }
  
  if(efficiency == TRUE) {
    range_confirmed = range_function(CC[["confirmed"]])
    range_unconfirmed = range_function(CC[["not_confirmed"]])
    return(list(PP, CC, range_confirmed, range_unconfirmed))
  } else {
    range = range_function(species)
    return(list(PP, CC, range))
  }
}



#Get the California Current System shape

CC_shape = mr_features_get(type = "MarineRegions:lme", featureID = "lme.10", format = "json")

#check that it's the right polygon
class(CC_shape) <- "mr_geojson"
CC_shape = mr_as_wkt(CC_shape)
pnt <- st_as_sfc(CC_shape, crs = 4326)
mapview(pnt)

#Simplify the polygon for use with GBIF
test = st_transform(pnt, crs = '+proj=aeqd +lat_0=53.6 +lon_0=12.7')
mapview(test)
test1 = st_simplify(test, preserveTopology = FALSE, dTolerance=10000)
test1 = st_transform(test1, crs = 4326)
mapview(test1)



#Simplify into a box for use with GBIF function
CC_shape = wkt2bbox(CC_shape)
CC_shape = c(CC_shape$min_x, CC_shape$min_y, CC_shape$max_x, CC_shape$max_y)



distribution = distribution_check(c(-122.50, 37.49, -122.49, 37.50), test1, species_standardized, 5, FALSE)

d = distribution
PP = d[[1]]$confirmed
CC = d[[2]]$confirmed
range = d[[3]]$confirmed
no_data = d[[3]]$no_data 

df_PP = c()
df_CC = c()
df_range = c()
df_no_data = c()
df_not_in_GBIF = c()


for (s in species_both$species) {
  if(is.na(s)) {
    df_not_in_GBIF = append(df_not_in_GBIF, TRUE)
    df_PP = append(df_PP, NA)
    df_CC = append(df_CC, NA)
    df_range = append(df_range, NA)
    df_no_data = append(df_no_data, NA)
    next
  } else {
    df_not_in_GBIF = append(df_not_in_GBIF, FALSE)
  }
  
  if(s %in% PP) {
    df_PP = append(df_PP, TRUE)
  } else {
    df_PP = append(df_PP, FALSE)
  }
   
  if(s %in% CC) {
    df_CC = append(df_CC, TRUE)
  } else {
    df_CC = append(df_CC, FALSE)
  }
  
  if(s %in% range) {
    df_range = append(df_range, TRUE)
  } else {
    df_range = append(df_range, FALSE)
  }
  
  if(s %in% no_data) {
    df_no_data = append(df_no_data, TRUE)
  } else {
    df_no_data = append(df_no_data, FALSE)
  }
  
}

GBIF_summary = data.frame(Species = species_both$verbatim_name, 
                     GBIFSpecies = species_both$species, 
                     PillarPointOccurrence = df_PP, 
                     CCSOccurrence = df_CC, 
                     Range = df_range, 
                     NoData = df_no_data, 
                     NotInGBIF = df_not_in_GBIF)

print(GBIF_summary %>% group_by(across(PillarPointOccurrence:NotInGBIF)) %>% summarise(count=n()))

##Add code to calculate the various percentages used in the text

write.csv(GBIF_summary, "Data/Analysis Products/RangeAnalysis.csv", row.names=TRUE)

```

Focused on just the `r samples` samples, we removed singletons and all `r overlap` ASVs that overlapped with negative controls (Supplemental Information X), and we rarefied samples to the minimum number of reads of any sample, leaving `r processed_ASVs` ASVs and `r processed_reads` reads. `r sprintf("%0.1f%%", unassigned_ASVs[unassigned_ASVs$Kingdom == "",]$freq * 100)` of ASVs were unassigned, and the remainder included representatives from `r phyla` phyla, `r class` classes, `r order` orders, `r family` families, `r genus` genera, and `r species` species (Figure 4). A breakdown of the relative proportions of reads and ASVs assigned to each phyla can be found in Table 1. To ensure the accuracy of these taxonomic identifications, we confirmed that 306 of `r species` identified species (73.7%) have occurrence records in the California Current System (CCS) and known ranges that encompass Pillar Point. (Supplemental Information X).

```{r Table 1}

 #merge all samples together
  #make abundance of each one
  #glom by phylum
  
  physeq_totalASVs = physeq_summed
  
  otu_table(physeq_totalASVs)[otu_table(physeq_totalASVs)>0] <- 1
  physeq_totalASVs = tax_glom(physeq_totalASVs, taxrank="Phylum", NArm=TRUE)
  
  
  Phyla_ASVs = data.frame(Phyla = tax_table(physeq_totalASVs)[,2], ASVs = rowSums(otu_table(physeq_totalASVs)))
  
  
  physeq_speciesbyphyla = physeq_summed
  physeq_speciesbytaxa = tax_glom(physeq_summed, taxrank="Species", NArm=TRUE)
  otu_table(physeq_speciesbytaxa)[otu_table(physeq_speciesbytaxa)>0] <- 1
  physeq_speciesbytaxa = tax_glom(physeq_speciesbytaxa, taxrank="Phylum", NArm=TRUE)
  
  Phyla_Species =  data.frame(Phyla = tax_table(physeq_speciesbytaxa)[,2], Species = rowSums(otu_table(physeq_speciesbytaxa)))

  
  list = list(Phyla_Reads, Phyla_ASVs, Phyla_Species)
  full = list %>% reduce(full_join, by='Phylum')
  full[is.na(full)] <- 0
  

  full_sorted = full[order(-full$Reads),]
  
  full_sorted = full_sorted %>% 
    mutate(Reads_Percent = scales::label_percent()(Reads/sum(Reads))) %>%
    mutate(ASVs_Percent = scales::label_percent()(ASVs/sum(ASVs))) %>%
    mutate(Species_Percent = scales::label_percent()(Species/sum(Species)))
  
  full_sorted$Phylum = remove_NCBInums(full_sorted$Phylum)
  full_sorted$Phylum = str_replace(full_sorted$Phylum, "phylum_", "Class: ")
  
  col_order = c("Phylum", "Reads", "Reads_Percent", "ASVs", "ASVs_Percent", "Species", "Species_Percent")
  
  full_sorted = full_sorted[,col_order]
  
  full_sorted
  
  write.csv(full_sorted, "Data/Analysis Products/TaxonDiversity.csv", row.names=TRUE)
```

```{r Figure 4 -- Phylogenetic Tree}

#Aggregate to species assignment and remove the NCBI identifiers
pt_physeq = tax_glom(data_physeq, taxrank="Species", NArm=FALSE)
tax_table(pt_physeq) = tax_table(pt_physeq)[!tax_table(pt_physeq)[,"Kingdom"]==""]

tax_table(pt_physeq)[tax_table(pt_physeq)==""] = NA
numbers = sub(".*_","", tax_table(pt_physeq))

taxa = apply(numbers, 1, function(x) tail(na.omit(x), 1))

tax_table(pt_physeq) = remove_NCBInums(tax_table(pt_physeq))


taxa_classification = classification(taxa, db = "ncbi")


taxa_tree = class2tree(taxa_classification, check = TRUE)

#Change the tip labels to match phyloseq objects (code only works if order is the same)
taxa_tree$phylo$tip.label = row.names(tax_table(pt_physeq))


phyla = unique(tax_table(pt_physeq)[,"Phylum"])


phyla_nodes = data.frame("node_phylum" = c(), "node" = c())  
phyla_nodes_secondary = data.frame("node_phylum" = c(), "tips" = c()) 

for (x in phyla) {
  OTUs = na.omit(row.names(tax_table(pt_physeq))[tax_table(pt_physeq)[,"Phylum"] == x])
  OTUs = as.vector(OTUs)
  num = ape::getMRCA(taxa_tree$phylo, tip = OTUs)
  if (is.null(num)) {
    nodes = as_tibble(taxa_tree$phylo)
    num = nodes$node[nodes$label == OTUs]
  } 
  df = data.frame("node_phylum" = c(x), "node" = c(num))
  phyla_nodes = rbind(phyla_nodes, df)
  
  if (length(OTUs) <6) {
    for (y in OTUs) {
      df2 = data.frame("node_phylum" = c(x), "tips" = c(y))
      phyla_nodes_secondary = rbind(phyla_nodes_secondary, df2)
    }
  }
}




pt_physeq = merge_phyloseq(pt_physeq, taxa_tree$phylo)


#Use Polychrome package to create a good color palette for the number of phyla
set.seed(567629)
colors = createPalette(28, c("#FFFFFF"), range = c(30, 80), target = c("normal", "protanope", "deuteranope", "tritanope"))

#Check whether the given order of colors has good contrast for different forms of colorblindness
pal.safe(colors)

#Manually re-order the colors to ensure there is good contrast between adjacent colors
colors_ordered = colors[c('NC2',
                          'NC11',
                          'NC27',
                          'NC25',
                          'NC4',
                          'NC5',
                          'NC8',
                          'NC24',
                          'NC9',
                          'NC10',
                          'NC12',
                          'NC13',
                          'NC1',
                          'NC14',
                          'NC15',
                          'NC16',
                          'NC7',
                          'NC17',
                          'NC18',
                          'NC19',
                          'NC20',
                          'NC21', 
                          'NC22',
                          'NC23',
                          'NC26',
                          'NC6',
                          'NC3',
                          'NC28')]

#Confirm that the new order works well
pal.safe(colors_ordered)


#Create ggtree
p <- ggtree(pt_physeq,  layout = "circular") 

ordered_names = get_taxa_name(p)
ordered_names = unique(ordered_names)
ordered_names[match(rownames(tax_table(pt_physeq)), ordered_names)] = tax_table(pt_physeq)[,"Phylum"]
ordered_names = unique(ordered_names)

names(colors_ordered) = ordered_names

p = p + geom_fruit(geom = geom_tile, mapping = aes(fill = Phylum), width = 3, show.legend = FALSE)  + scale_fill_manual(values = colors_ordered) +scale_color_manual(values = colors_ordered)

p = p +  geom_fruit(data=phyla_nodes_secondary, geom = geom_tile, mapping = aes(y = tips, fill = node_phylum), width = 6, show.legend = FALSE)  

p = p + geom_hilight(data=phyla_nodes, mapping=aes(node = node, fill = node_phylum), show.legend = FALSE)

p = p + geom_cladelab(data=phyla_nodes, mapping=aes(node=node, label= node_phylum), geom = "text", angle = "auto", offset = 5, fontsize=2, barcolor=NA, show.legend = FALSE)

p = p +
  theme(
    panel.background = element_rect(fill='transparent'),
    plot.background = element_rect(fill='transparent', color=NA),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.background = element_rect(fill='transparent'),
    legend.box.background = element_rect(fill='transparent')
  )

ggsave(
  "Figures/Figure4.pdf",
  plot = p,
  dpi = 300,
  bg='transparent')

```
```{r Phylogenetic Tree for PowerPoint}

p <- ggtree(pt_physeq,  layout = "rectangular") 

ordered_names = get_taxa_name(p)
ordered_names = unique(ordered_names)
ordered_names[match(rownames(tax_table(pt_physeq)), ordered_names)] = tax_table(pt_physeq)[,"Phylum"]
ordered_names = unique(ordered_names)

names(colors_ordered) = ordered_names

p = p + geom_fruit(geom = geom_tile, mapping = aes(fill = Phylum), width = 3, show.legend = FALSE)  + scale_fill_manual(values = colors_ordered) +scale_color_manual(values = colors_ordered)

p = p + geom_hilight(data=phyla_nodes, mapping=aes(node = node, fill = node_phylum), show.legend = FALSE)



p = p +
  theme(
    panel.background = element_rect(fill='transparent'),
    plot.background = element_rect(fill='transparent', color=NA),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.background = element_rect(fill='transparent'),
    legend.box.background = element_rect(fill='transparent')
  )

ggsave(
  "Figures/PowerPointFigure/phylo.pdf",
  plot = p,
  width = 4.49,
  height = 7.06,
  dpi = 300,
  bg='transparent')
```


#### 3.2 Individual-Level Differences Between Locations

```{r}

#Subset ampvis2 data by location

amp_S1 = amp_filter_samples(data_amp, Location %in% c("S1"))
amp_S2 = amp_filter_samples(data_amp, Location %in% c("S2"))
amp_N = amp_filter_samples(data_amp, Location %in% c("N"))

#List the unique taxa at each location

S1_taxa = unique(amp_S1$tax$Species)
S1_taxa = S1_taxa[S1_taxa != ""]
S2_taxa = unique(amp_S2$tax$Species)
S2_taxa = S2_taxa[S2_taxa != ""]
N_taxa = unique(amp_N$tax$Species)
N_taxa = N_taxa[N_taxa != ""]

#List the unique ASVs at each locaton

S1_ASV = unique(amp_S1$tax$OTU)
S2_ASV = unique(amp_S2$tax$OTU)
N_ASV = unique(amp_N$tax$OTU)


#Using eulerr, create euler diagrams for taxa and ASVs


add_quantity_line_break = function(v) {
  tags <- v$children$canvas.grob$children$diagram.grob.1$children$tags$children
  
  tags <- do.call(grid::gList, lapply(tags, function(x) {
    x$children[[2]]$label <- sub(" \\%)", "%)", x$children[[2]]$label)
    before = sub("\\(.*", "", x$children[[2]]$label)
    after = gsub(".*\\(", "(", x$children[[2]]$label)
    italics = bquote(atop(.(before),italic(.(after))))
    x$children[[2]]$label <- italics
    x$children[[2]]$just <- NULL
    x}))
  
  v$children$canvas.grob$children$diagram.grob.1$children$tags <- tags
  
  return(v)
}


E_taxa <- euler(list(S1 = S1_taxa, S2 = S2_taxa, N = N_taxa), shape = "ellipse")
E_taxa_plot = plot(E_taxa, quantities = list(type = c("counts", "percent"), cex=2), legend = list(side ="right", nrow = 1, ncol = 3, cex = 2.5))
E_taxa_plot = add_quantity_line_break(E_taxa_plot)


E_ASV <- euler(list(S1 = S1_ASV, S2 = S2_ASV, N = N_ASV), shape="ellipse")
E_ASV_plot = plot(E_ASV, quantities = list(type = c("counts", "percent"), cex=2), legend = list(side ="right", nrow = 1, ncol = 3, cex = 2.5))
E_ASV_plot = add_quantity_line_break(E_ASV_plot)

```

Across both taxa (n=415) and ASVs (n=27,038), all three locations had unique elements, suggesting that there was not complete mixing of eDNA across micro-habitats. 33.2% of taxa and 80% of ASVs were unique to one of the three locations (Figure 5). At both the taxa and ASV level, S1 had more unique elements than S2 and nearshore, although this effect was more pronounced across ASVs. ASVs and taxa shared across pairs of two locations were the least common elements. 

```{r}

(wrap_elements(panel = E_taxa_plot$children$canvas.grob) + wrap_elements (panel = E_ASV_plot$children$canvas.grob)) / wrap_elements(panel = E_taxa_plot$children$legend.grob) + plot_layout(heights = c(7,1))

```



```{r, warning = FALSE}

#Function for creating indicator species heatmap from physeq data

indicator_heatmap <- function(physeq, asv) {
  
  #If analyzing by taxa, remove all unclassified ASVs and condense to species assignments
  if(asv == FALSE) {
    physeq = tax_glom(physeq, taxrank="Species", NArm=TRUE)
    tax_table(physeq) = gsub("\\_.*","",tax_table(physeq))
  }
  
  #Convert to presence/absence data
  otu_table(physeq)[otu_table(physeq) >0] <- 1
  
  
  #Use indicspecies package to get top taxa
  indval = multipatt(t(otu_table(physeq)),t(sample_data(physeq)[,"Location"]), control = how(nperm=999), duleg = TRUE)
  top_N = rownames(indval$sign %>% filter(p.value < 0.05) %>% filter(s.N == 1)  %>% filter(stat > .7))
  top_S1 = rownames(indval$sign %>% filter(p.value < 0.05) %>% filter(s.S1 == 1)  %>% filter(stat > .7))
  top_S2 = rownames(indval$sign %>% filter(p.value < 0.05) %>% filter(s.S2 == 1)  %>% filter(stat > .7))
  
  indicators_all = c(top_N, top_S1, top_S2)
  
  #Merge samples by site/time before plotting 
  physeq_merge = merge_samples(physeq, "SiteByTime")
  
  #Update metadata accordingly
  updated_metadata = as.matrix(sample_data(physeq)[, c("SiteByTime", "Time", "Location", "T", "S")])
  updated_metadata = as.data.frame(updated_metadata)
  updated_metadata$SiteByTime = as.factor(updated_metadata$SiteByTime)
  updated_metadata$Time = as.factor(updated_metadata$Time)
  updated_metadata$Location = as.factor(updated_metadata$Location)
  updated_metadata = distinct(updated_metadata)
  rownames(updated_metadata) = updated_metadata[,'SiteByTime']
  sample_data(physeq_merge) <- sample_data(updated_metadata)

  #Divide all merged samples by the number of samples they represent 
  otu_table(physeq_merge) = otu_table(physeq_merge)/3
  
  otu_table(physeq_merge)[grepl('11:30', rownames(otu_table(physeq_merge)))] = otu_table(physeq_merge)[grepl('11:30', rownames(otu_table(physeq_merge)))]/3
  
  otu_table(physeq_merge)[grepl('14:00', rownames(otu_table(physeq_merge)))] = otu_table(physeq_merge)[grepl('14:00', rownames(otu_table(physeq_merge)))]/3
 
   otu_table(physeq_merge)[grepl('17:00', rownames(otu_table(physeq_merge)))] = otu_table(physeq_merge)[grepl('17:00', rownames(otu_table(physeq_merge)))]/3
  
  #Add labels to the indicator species for plotting
  indicators_only <-prune_taxa(colnames(otu_table(physeq_merge)) %in% indicators_all, physeq_merge)
  
  add_indicator_label <- function(X) {
    if (X %in% top_N) { return("N")}
    else if (X %in% top_S1) { return("S1")}
    else if (X %in% top_S2) {return ("S2")}
    else {return("NA")}
  }
  
  indicators<- sapply(rownames(tax_table(indicators_only)), add_indicator_label)

  tax_table(indicators_only) = cbind(tax_table(indicators_only), indicators)

  #Plot
  plot = plot_heatmap(indicators_only, 
                      method = NULL, 
                      distance = NULL, 
                      trans = NULL, 
                      taxa.order = rev(indicators_all), 
                      taxa.label = "Species", 
                      sample.label = "Time", 
                      sample.order = "Time") + 
    facet_grid(factor(indicators, levels=c('S1', 'S2', 'N'))~factor(Location, levels=c('S1', 'S2', 'N')), 
               scales="free", 
               space = "free", 
               switch = "y") +
    scale_fill_gradient(low = "black", high = "#B5D7E4") + 
    theme_grey(base_size = 18) + 
    theme(plot.title = element_text(hjust = 0.5)) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) +
    theme(strip.placement = "outside", axis.text.x=element_text(size=12))  +
    theme(strip.text = element_text(size = rel(1))) +
    theme(legend.text=element_text(size=12), legend.key.height = unit(1.5, "cm")) +
    theme(panel.background = element_blank(), panel.grid.major = element_blank()) +
    labs(fill = "Proportion of \nPositive Detects", title = "Locations") +
    theme(axis.ticks = element_line(color = "black"),
          axis.text.y = element_text(color="black", face = "italic"), 
          axis.text.x = element_text(color="black")) 
  
  plot$scales$scales[[2]]$name <- "Indicators"
  
  return(list(plot = plot, 
              top_N = top_N, 
              top_S1 = top_S1, 
              top_S2 = top_S2, 
              all_indicators = indicators_all, 
              full_output = indval))
}

heatmap_taxa = indicator_heatmap(data_physeq, FALSE)
heatmap_ASV = indicator_heatmap(data_physeq, TRUE)

indicator_ASVs = data_amp$tax[heatmap_ASV$all_indicators,]
unassigned_indicator_ASVs = indicator_ASVs %>% group_by(Kingdom) %>% summarise(n = n()) %>% mutate (freq = n / sum(n))
  
```


```{r Figure 6}

heatmap_taxa$plot


#ADD FIGURE SAVING

```

```{r Figure for PowerPoint}

 graph = heatmap_taxa$plot 

  path = "Figures/PowerPointFigure/indicator_figure.png"
  ggsave(filename = path, plot = graph, width = 13, height = 6.69)
  

```

```{r Supplemental Figure X}

heatmap_ASV$plot

```

Using an indicator species framework produced similar trends to the analysis of unique taxa and ASVs; all three locations had indicator taxa and ASVs, and S1 had more indicator elements than other locations. By taxa, we identified `r length(heatmap_taxa$all_indicators)` indicator taxa:  `r length(heatmap_taxa$top_S1)` for S1, `r length(heatmap_taxa$top_S2)` for S2, and `r length(heatmap_taxa$top_N)` for nearshore (Figure 6). By ASVs, we found more indicators overall. We identified `r length(heatmap_ASV$all_indicators)` indicator ASVs: `r length(heatmap_ASV$top_S1)` for S1, `r length(heatmap_ASV$top_S2)` for S2, and `r length(heatmap_ASV$top_N)` for nearshore (Supplemental Figure X). Most (`r sprintf("%0.1f%%", unassigned_indicator_ASVs[unassigned_indicator_ASVs$Kingdom == "",]$freq * 100)`) indicator ASVs had no taxonomic assignment, but the ones that did included several species beyond the original indicator taxa, at S1 (Scorpaenichthys marmoratus, Corallina confusa, Leathesia difformis, Bossiella frondescens, Myrionema balticum, Bossiella plumosa), S2 (Bossiella frondescens, Smithora naiadum), and nearshore (Mazzaella splendens, Strongylocentrotus purpuratus, Halichondria panicea, Smithora naiadum). 

#### 3.3.	Community-Level Differences between Locations

```{r}
#Calculate Jaccard dissimilarity matrices
jaccard_full = vegdist(t(data_amp$abund), binary = TRUE, method = "jaccard")
jaccard_taxa = vegdist(t(data_amp_taxa$abund), binary = TRUE, method = "jaccard")
```

```{r}

#Calculate jaccard dissimilarity 

set.seed(0)

#PERMANOVA on full data set
permutest(betadisper(jaccard_full, data_amp$metadata$Location))
adonis2(jaccard_full ~ Location + Time + Dereplicated_Sample_Name , data = data_amp$metadata)

#PERMANOVA on taxa data set
permutest(betadisper(jaccard_taxa, data_amp_taxa$metadata$Location))
adonis2(jaccard_taxa ~ Location + Time + Dereplicated_Sample_Name, data = data_amp_taxa$metadata)

```

```{r, message=FALSE, echo=FALSE, cache = TRUE}

#NMDS

#Check the stress vs. dimension plot

# Function that performs a NMDS for 1-10 dimensions and plots the nr of dimensions vs the stress
NMDS.scree <- function(x) { #where x is the name of the data frame variable
  plot(rep(1, 10), replicate(10, metaMDS(x, autotransform = F, k = 1)$stress), xlim = c(1, 10),ylim = c(0, 0.30), xlab = "# of Dimensions", ylab = "Stress", main = "NMDS stress plot")
  for (i in 1:10) {
    points(rep(i + 1,10),replicate(10, metaMDS(x, autotransform = F, k = i + 1)$stress))
  }
}

# Use NMDS.scree function to choose the optimal nr of dimensions
NMDS.scree(jaccard_full)
NMDS.scree(jaccard_taxa)


#Establish functions for PAM clustering algorithm

PAM_validation <- function(dist) {
  
  k_list <- c()
  avg_sil_width <- c(0)
  for (x in 1:(nrow(as.matrix(dist))-1)){
    PAM = pam(dist, k=x, diss = TRUE)
    avg_sil_width <- c(avg_sil_width, PAM$silinfo$avg.width)
    k_list <- c(k_list, x)
  }
  
  PAM_validation <- data.frame (k = k_list, sil_width = avg_sil_width, x_max = which.max(avg_sil_width))
  return(PAM_validation)
}

PAM_validation_viz <- function(PAM_validation) {
  plot = ggplot(data = PAM_validation, 
                aes(x=k, y = sil_width)) + 
    geom_line() + 
    geom_point() + 
    xlab("K") + 
    ylab("Average Silhouette Width") + 
    ggtitle("Cluster Validation using Average Silhouette Width") + 
    geom_segment(aes(x= x_max[1], 
                     y = 0, 
                     xend = x_max[1], 
                     yend = sil_width[x_max[1]], 
                     color = "red"), 
                 linetype = 2, 
                 show.legend = FALSE)
  return(plot)
}

PAM_silhouette_viz_3 <- function(PAM, sample_data) {
  PAM_df = data.frame(PAM$silinfo$widths)
  PAM_df <- cbind(PAM_df, X = row.names(PAM_df))
  PAM_df <- merge(PAM_df, sample_data, by = "X")
  PAM_df$cluster <- as.factor(PAM_df$cluster)
  
  ggplot(data = PAM_df, 
         aes(x=sil_width, y = reorder(X, sil_width), fill = Location)) + 
    geom_point(aes(shape=Location, color=Time), size = 3, position=position_jitter (height = 30, width = 0)) + 
    scale_shape_manual(values = c(16, 15, 17)) + 
    facet_grid(cluster ~ ., scales = "free_y", space = "free_y") +
    viridis::scale_color_viridis(discrete=TRUE, begin = 0, end = .97, direction = -1) + 
    xlab("Silhouette Width") + 
    ylab("") + 
    geom_vline (xintercept = PAM$silinfo$avg.width, linetype = "dashed", color = "red") + 
    theme_classic(base_size = 14) + 
    theme(axis.text.x = element_text(angle = 90)) + 
    ggtitle("PAM Clustering Visualization") + 
    theme(axis.text.y = element_blank(), 
          axis.ticks.y = element_blank(), 
          panel.grid.major = element_blank(), 
          panel.grid.minor = element_blank(),
          panel.background = element_blank(), 
          panel.border=element_rect(colour="black",size=1, fill = NA)) + 
    scale_y_discrete(expand = expansion(add = 40))
}


NMDS_PAM <- function(amp, jaccard, dim, PAM_logical) {
  set.seed(0)
  NMDS = metaMDS(jaccard, dim, trymax = 100, tidy = TRUE)
  
  NMDS_stress = stressplot(NMDS)
  
  data.scores <- as.data.frame(scores(NMDS))
  data.scores$X <- rownames(data.scores)
  data.scores <- merge(data.scores, amp$metadata)
  
  annotations1 <- data.frame(
    xpos = c(Inf), ypos =  c(-Inf), 
    annotateText = c(paste("Stress:", round(NMDS$stress, 3))),
    hjustvar = c(1),  
    vjustvar = c(-.5))
  
  plot1 = ggplot(data=data.scores, aes(x=NMDS1, y=NMDS2)) + 
    theme_light() + 
    geom_point(aes(shape=Location, color=Time), size = 3) + 
    scale_shape_manual(values = c(16, 15, 17)) + 
    theme(text = element_text(size = 14)) +
    ggtitle ("A) nNMDS Ordination (Jaccard Dissimilarity)") +
    viridis::scale_color_viridis(discrete=TRUE, begin = 0, end = .97, direction = -1) +
    stat_ellipse(aes(group=Location), type = "t",linetype = 2, alpha = 1) 
  

  plot1 = plot1 + 
    theme_classic(base_size = 14) + 
    geom_text(data = annotations1, aes(x=xpos, y=ypos, hjust = hjustvar, vjust=vjustvar, label = annotateText ))
  
   if(PAM_logical == TRUE) {
  validated_k = PAM_validation(jaccard)
  PAM_validation = PAM_validation_viz(validated_k)
  
  PAM = pam(jaccard, k=validated_k$x_max[1], diss = TRUE)
  plot2 = PAM_silhouette_viz_3(PAM, amp$metadata) 
  
  return(list(NMDS = plot1, PAM = plot2, NMDS_stress = NMDS_stress, PAM_validation = PAM_validation))
  
}

  return(plot1)
}



NMDS_PAM_full = NMDS_PAM(data_amp, jaccard_full, 2, TRUE)
NMDS_PAM_full$NMDS_stress
NMDS_PAM_full$PAM_validation


NMDS_PAM_taxa = NMDS_PAM(data_amp_taxa, jaccard_taxa, 2, TRUE)
NMDS_PAM_taxa$NMDS_stress
NMDS_PAM_taxa$PAM_validation

```

Community composition also differed across locations, as supported by multiple analyses. Variation in community composition was significantly explained by location, even when accounting for variation due to time of sampling and variation between replicates (ASVs: PERMANOVA  p < 0.001, betadisper p > 0.5; Taxa: PERMANOVA  p < 0.001, betadisper p > 0.5). As visualized using NMDS ordination, S1 and S2 samples collected around low tide were most dissimilar from the offshore samples (Figures 7A, 7C). These patterns persisted in the optimal clusters identified using the partitioning among medoids (PAM) algorithm, without assuming a priori that location drives the clusters (Figure 7B, 7D). When analyzed by ASV, PAM identified three optimal clusters, primarily composed of: 1) nearshore samples with additional samples from S1 and S2 at early time points, 2) the remaining S1 samples, and 3) the remaining S2 samples (Figure 7B). When analyzing by taxa, PAM additionally differentiated between samples from the three locations collected at early time points. Thus, multiple analyses with different initial assumptions all demonstrated differentiable eDNA signals across micro-habitats. 

```{r Figure 7}

(NMDS_PAM_full$NMDS + theme(legend.position = "none") + ggtitle("A)")) + 
  (NMDS_PAM_full$PAM  + theme(legend.position = "none") + ggtitle("B)")) + 
  (NMDS_PAM_taxa$NMDS + theme(legend.position = "none") + ggtitle("C)")) + 
  (NMDS_PAM_taxa$PAM + ggtitle("D)")) + 
  plot_layout(guides = 'collect', widths = c(1,2))

#Add figuring saving
```

```{r Animated Figure for PowerPoint}

alpha=c(0,0,0,0,0,0,0,0,0,0,0,0)
graph = NMDS_PAM_full$NMDS + 
  ggtitle("NMDS Ordination (Jaccard Dissimilarity)") +
  theme_classic(base_size = 18)

filename = paste("NMDS_figure_full.pdf")
  path = paste("Figures/PowerPointFigure/", filename)
  pdf(path, width = 11.97, height = 6.14)
  print(graph)
  dev.off()
  
  graph$layers[[2]] <- NULL
  
  graph = graph + 
    stat_ellipse(aes(group=Location), type = "t",linetype = 2, alpha = 0) 
  
for (i in 1:12) {
  alpha[i] = 1
  graph <- graph + 
    viridis::scale_color_viridis(discrete=TRUE, 
                                 begin = 0, 
                                 end = .97, 
                                 direction = -1, 
                                 alpha = alpha) 
  filename = paste("NMDS_figure", i, ".pdf")
  path = paste("Figures/PowerPointFigure/", filename)
  pdf(path, width = 11.97, height = 6.14)
  print(graph)
  dev.off()
  alpha[i] = .2
}



```






```{r}

distmat <- as.matrix(jaccard_full)

#Calculate Pairwise Dissimilarity Values Within and Across Sites

Jaccard = c()
Combo = c()
Time = c()
summary = data.frame(Jaccard, Combo, Time)

one = c("S1", "S2", "O", "S1", "S2", "S1")
one_names = c("S1", "S2", "N", "S1", "S2", "S1")
two = c("S1", "S2", "O", "O", "O", "S2")
two_names = c("S1", "S2", "N", "N", "N", "S2")

for(x in levels(data_amp$metadata$Time)){
  subset = filter(data_amp$metadata, data_amp$metadata$Time == x)
  
  for(i in 1: length(one)) {
    name = paste(one_names[i], two_names[i], sep = ":")
    data = distmat[subset$X[subset$Location == one[i]], subset$X[subset$Location == two[i]]]
    Jaccard = vector()
    
    if (!(0 %in% dim(data))) {
      for (row in 1:nrow(data)) {
        for (col in 1:ncol(data)){
           if (col > row) {
            Jaccard = c(Jaccard, data[row,col])
          }
        }
      }
    }
    Combo <- rep(name, length(Jaccard))
    Time <- rep(x, length(Jaccard))
    
    summary <- rbind(summary, data.frame(Jaccard, Combo, Time))
  }
}


##PLOTTING

#Define palette for colors and fills 
colors = turbo(8)

palette = c("Within Site" = "#30123BFF", 
            "S1:N" ="#1AE4B6FF" , 
            "S2:N" = "#FABA39FF", 
            "S1:S2" = "#7A0403FF")

fill_palette = c("Within Site" = "#30123B20", 
                 "S1:N" ="#1AE4B620", 
            "S2:N" = "#FABA3920", 
            "S1:S2" = "#7A040320")


#WITHIN SITE COMBINED

summary$Combo2 <- summary$Combo
summary$Combo2[summary$Combo2 == "S1:S1"] = "Within Site"
summary$Combo2[summary$Combo2 == "S2:S2"] = "Within Site"
summary$Combo2[summary$Combo2 == "N:N"] = "Within Site"

stats = summary %>% filter(Combo2 == "Within Site") %>% summarize(mean = mean(Jaccard), max = max(Jaccard))


g2.5 = ggplot(summary, aes(x = Time, 
                           y = Jaccard, 
                           color = factor(Combo2, level = c("Within Site", "S1:N", "S2:N", "S1:S2")), 
                           fill = factor(Combo2, level = c("Within Site", "S1:N", "S2:N", "S1:S2")))) + 
  geom_boxplot(position = position_dodge(.5, preserve = "single")) + 
  geom_point(position = position_jitterdodge(dodge.width = .5), alpha=0.2) + 
  scale_color_manual(values = palette, name = "Site Pairings") + 
  scale_fill_manual(values = fill_palette, name = "Site Pairings") + 
  geom_hline(yintercept = stats$mean, color = palette["Within Site"] ) + 
  ylab("Jaccard Dissimilarity") +
  coord_cartesian(expand = 0, clip = "off") + 
  geom_text(aes(max(Time),
                stats$mean, 
                label = "Mean Dissimilarity\n  Within Sites", 
                hjust = -.2), 
            color = palette["Within Site"], 
            show.legend = FALSE, 
            check_overlap = TRUE) +
  theme_classic(base_size = 16) + 
  theme(axis.ticks = element_line(color = "black"),
        axis.text.y = element_text(color="black"), 
        axis.text.x = element_text(color="black"))

```

The extent of the Jaccard dissimilarity between samples from different locations varied over time. As shown in Figure 8, except at the first time point sampled, the dissimilarity between samples across sites was always greater than the mean dissimilarity between samples within the same site, and increasingly so across the period sampled. At two time points—15:30 and 16:00—after low tide but before water was moving between all locations again, all dissimilarity values across sites were greater than the maximum dissimilarity recorded between two samples from the same site. 

```{r Figure 8}

g2.5

#ADD FIGURE SAVING

```

#### 3.4.	Ecological Significance

```{r}

S1_unique = setdiff(S1_taxa, union(S2_taxa,N_taxa) ) 
S2_unique = setdiff(S2_taxa, union(S1_taxa,N_taxa) ) 
N_unique = setdiff(N_taxa, union(S2_taxa,S1_taxa) ) 


S1 = data.frame(list(Species = remove_NCBInums(S1_unique), Unique = "S1"))
S2 = data.frame(list(Species = remove_NCBInums(S2_unique), Unique = "S2"))
N = data.frame(list(Species = remove_NCBInums(N_unique), Unique = "N"))
unique = rbind(S1, S2, N)

S1_i = data.frame(list(Species = remove_NCBInums(data_amp$tax$Species[rownames(data_amp$tax) %in% heatmap_taxa$top_S1]), Indicator = "S1"))
S2_i = data.frame(list(Species = remove_NCBInums(data_amp$tax$Species[rownames(data_amp$tax) %in% heatmap_taxa$top_S2]), Indicator = "S2"))          
N_i = data.frame(list(Species = remove_NCBInums(data_amp$tax$Species[rownames(data_amp$tax) %in% heatmap_taxa$top_N]), Indicator = "N")) 
indicators = rbind(S1_i, S2_i, N_i)

df_list = list(unique, indicators)
full_taxa_list = df_list %>% reduce(full_join, by='Species')


#Get synonyms for taxa from WoRMS
#to add: (get synonyms via taxize?)
#to add: process synonyms to a) remove any parentheticals, and b) remove anything beyond two words

synonyms = c()
for (s in full_taxa_list$Species) {
  print(s)
  worms_ID = get_wormsid(s)
  if(is.na(worms_ID)) {
    syns = NA
  } else {
    syns = synonyms(worms_ID, db = 'worms')
    syns = toString(syns[[1]]$scientificname)
  }
  synonyms = rbind(synonyms, syns)
}

full_taxa_list = cbind(full_taxa_list, synonyms)

write.csv(full_taxa_list, "Data/Analysis Products/BPT_Analysis.csv", row.names=FALSE)

#make combined PDF of whole book
#for combined PDF, find every page number where names occur
#in each group of PDFs, search for all the names in the list

#Ecological Significance 
#Add section here where I import/read data rather than just pull the numbers

S1_hm = 12
S1_no_hm = 5

S2_hm = 2
S2_no_hm = 12

N_hm = 2
N_no_hm = 7

S1 = c(Yes = S1_hm, No = S1_no_hm)
S2 = c(Yes = S2_hm, No = S2_no_hm)
N = c(Yes = N_hm, No = N_no_hm)

counts = rbind(S1, S2, N)

test = chisq.test(counts, correct = FALSE, simulate.p.value = TRUE)



test_post_hoc = chisq.posthoc.test(counts, method = "bonferroni", simulate.p.value = TRUE)

```

Identified unique and indicator taxa reflected known ecological differences between locations. Only a subset of unique and indicator taxa were described in Between Pacific Tides: 18.8% (9/48) of nearshore taxa, 32.1% (17/53) of S1 taxa, and 31.1% (14/45) of S2. However, across the described subset, more taxa from S1 were categorized to high and middle intertidal zones than S2 and N (Figure 9; not made in R). The proportion of taxa from the high and middle intertidal varied significantly across locations (χ2 = 11.67, p < 0.05), matching the environmental characteristics of the sites.  



### Session Info

```{r}
sessionInfo()
```
